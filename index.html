<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="utf-8">
<title>University of Pittsburgh NLP Seminar</title>
<meta name="viewport" content="width=device-width, initial-scale=1.0" />
<meta name="description" content="" />
<meta name="author" content="http://webthemez.com" />
<!-- css -->
<link href="css/bootstrap.min.css" rel="stylesheet" />
<link href="css/fancybox/jquery.fancybox.css" rel="stylesheet">
<link href="css/jcarousel.css" rel="stylesheet" />
<link href="css/flexslider.css" rel="stylesheet" />
<link href="js/owl-carousel/owl.carousel.css" rel="stylesheet">
<link href="css/style.css" rel="stylesheet" />

<!-- HTML5 shim, for IE6-8 support of HTML5 elements -->
<!--[if lt IE 9]>
      <script src="http://html5shim.googlecode.com/svn/trunk/html5.js"></script>
    <![endif]-->

</head>
<body>
<div id="wrapper">
	<!-- start header -->
	<header>
	    <div class="container">
		 <a class="text-center" href="index.html">
       <img src="img/University_of_Pittsburgh_Logo_RGB_Primary_3-Color.png" alt="logo" style="width:25%;height:auto;padding-bottom:10px;margin-bottom:10px;margin-top:10px;margin-right:50px"/>
		  </a>
      <strong style="font-size:16pt;margin-left:10%">Natural Language Processing Seminar</strong>
	    </div>
	</header>
	<!-- end header -->
	<section id="inner-headline">
	</section>

	<section id="content">


	<div class="container" >

		<div class="skill-home">
		<div class="skill-home-solid clearfix" >
			<div style="text-align:center;font-size:16pt;font-weight:bold">
				This Week's Speaker
			</div>
			<hr>
			<hr>

			<div class="row">
			<div class="col-md-12 text-left">
				<div class="speaker-pic" style="display: inline-block">
					<img src="img/ChaiPhoto.jpg">
				</div>
				<div>
				<h3> Joyce Chai </h3>
				<p><b> University of Michigan </b></p>
				<p><b> Time:</b> 4/20/2021, 1pm EST</p>
				<p><b> Place:</b> Zoom</p>
				<p><b> Title:</b> Language Communication with Robots</p>
				<p><b> Topic:</b> Human-Robot Communication </p>
				<p><b>Abstract:</b> With the emergence of a new generation of cognitive robots, enabling natural communication between humans and robots has become increasingly important. Humans and robots are co-present in a shared physical environment; however, they have mismatched capabilities in perception, action, and reasoning. They also have different levels of linguistic, world, and task knowledge. All of these lead to a significant disparity in their representations of the shared world, which makes language communication difficult. In this talk, I will introduce some effort in my lab that addresses these challenges, particularly in the context of communicative learning where humans can teach agents the shared environment and tasks through language communication. I will talk about collaborative models for grounded language processing which are motivated by cooperative principles in human communication. I will highlight the role of physical causality in grounding language to perception and action and discuss key challenges and opportunities.
				<p><b>Bio:</b> Joyce Chai is a Professor in the Department of Electrical Engineering and Computer Science at the University of Michigan. Prior to joining UM in 2019, she was a Professor in Computer Science and Engineering at Michigan State University. Her research interests include natural language processing, situated and embodied dialogue, and human-AI communication and collaboration. Her recent work focuses on grounded language processing to facilitate natural communication with robots and other artificial agents. She is Associate Editor for Journal of Artificial Intelligence Research (JAIR) and ACM Transaction and Intelligent Interactive Systems (TiiS), and most recently served as Program Co-chair for the Annual Meeting of Association for Computational Linguistics (ACL) in 2020.  She is the receipt of the National Science Foundation CAREER Award (2004), the Best Long Paper Award from Association of Computational Linguistics (2010), and the William Beal Outstanding Faculty Award at MSU (2018).  She holds a Ph.D. in Computer Science from Duke University.
				</p>
				</div>
			</div>
			</div>

			<hr>
			<hr>
			<div style="text-align:center;font-size:16pt;font-weight:bold">
				Previous Speakers
			</div>
			<hr>
			<hr>
			<div class="row">
			<div class="col-md-12 text-left">
				<!--<div class="col-md-4 offset-md-8" >-->
					<div class="speaker-pic" style="display: inline-block">
						<img src="https://people.ischool.berkeley.edu/~dbamman/bamman_pic_large.jpg">
					</div>
				<!--</div>-->
				<div>
				<h3> David Bamman</h3>
				<p><b> University of California, Berkeley </b></p>
				<p><b> Time:</b> 10/13/2020, Tuesday from 3 to 4 pm</p>
				<p><b> Place:</b> Zoom </p>
				<p><b> Title:</b> Modeling the Spread of Information within Novels </p>
				<p><b> Topic:</b> Language Generation </p>
				<p><b> Abstract:</b> Understanding the ways in which information flows through social networks is important for questions of influence--including tracking the spread of cultural trends and disinformation and measuring shifts in public opinion.  Much work in this space has focused on networks where nodes, edges and information are all directly observed (such as Twitter accounts with explicit friend/follower edges and retweets as instances of propagation); in this talk, I will focus on 	the comparatively overlooked case of information propagation in *implicit* networks--where we seek to discover single instances of a message passing from person A to person B to person C, only given a depiction of their activity in text. </p>
				<p>	Literature in many ways presents an ideal domain for modeling information propagation described in text, since it depicts a largely closed universe in which characters interact and speak to each other.  At the same time, it poses several wholly distinct challenges--in particular, both the length of literary texts and the subtleties involved in extracting information from fictional works pose difficulties for NLP systems optimized for other domains.  In this talk, I will describe our work in measuring information propagation in these implicit networks, and detail an NLP pipeline for discovering it, focusing in detail on new datasets we have created for tagging characters and their coreference in text.  This is joint work with Matt Sims, Olivia Lewke, Anya Mansoor, Sejal Popat and Sheng Shen. </p>
				<p><b> Bio:</b> David Bamman is an assistant professor in the School of Information at UC Berkeley, where he works in the areas of natural language processing and cultural analytics, applying NLP and machine learning to empirical questions in the humanities and social sciences. His research focuses on improving the performance of NLP for underserved domains like literature (including LitBank and BookNLP) and exploring the affordances of empirical methods for the study of literature and culture. Before Berkeley, he received his PhD in the School of Computer Science at Carnegie Mellon University and was a senior researcher at the Perseus Project of Tufts University. Bamman's work is supported by the National Endowment for the Humanities, National Science Foundation, an Amazon Research Award, and an NSF CAREER award. </p>
				<p><b> Slides: </b><a href="slides/pitt_nlp.pdf">Download </a></p>
				</div>
			</div>
		</div>

		<hr>

			<div class="row">

			<div class="col-md-12 text-left">
				<div class="speaker-pic" style="display: inline-block">
					<img src="https://cocoxu.github.io/files/weixu_profile.png">
				</div>

				<div class="speaker-pic" style="display: inline-block">
				</div>
				<div>
				<h3> Wei Xu </h3>
				<p><b> Georgia Institute of Technology</b></p>
				<p><b> Time:</b> 10/27/2020, Tuesday from 3 to 4 pm </p>
				<p><b> Place:</b> Zoom </p>
				<p><b> Title:</b> Automatic Text Simplification for K-12 Students </p>
				<p><b> Topic:</b> Language Generation </p>
				<p><b> Abstract:</b> Reading and writing are fundamental to the learning experience of students. In this talk, I will first exemplify how professional editors rewrite news articles to meet readability standards of elementary and middle schools, then demonstrate how we can develop new machine learning models to mimic the human editing process. </p>
					<p> I aim to answer four research questions: (1) How to create a parallel corpus for training neural text generation models? (2) How to design neural generation models with better controllability? (3) How to automatically evaluate system generated text outputs? (4) How to estimate the readability at the word- and phrase-level more reliably? </p>
					<p> On the high-level, we designed a neural Conditional Random Fields model to automatically align sentences between the complex and simplified articles, and consequently, created two large text simplification corpora (Newsela-Auto and Wiki-Auto). We also proposed a novel hybrid approach that leverages linguistically motivated rules for splitting and deletion, and couples with a neural paraphrasing model to produce varied rewriting styles. SARI, a tunable automatic evaluation metric, has been used for system comparison in addition to human evaluation. As for readability assessment, we improved the state-of-the-art by using a pairwise neural ranking model in conjunction with a manually rated word-complexity lexicon. </p>
				<p><b>Bio:</b> Wei Xu is an assistant professor in the School of Interactive Computing at the Georgia Institute of Technology. Before joining Georgia Tech, she was an assistant professor at Ohio State University since 2016. Xuâ€™s research interests are in natural language processing, machine learning, and social media. Her recent work focuses on text generation, semantics, information extraction, and reading assistive technology. She has received the NSF CRII Award, Best Paper Award at COLING, CrowdFlower AI for Everyone Award, and Criteo Faculty Research Award.</p>
				</div>
				<p><b> Slides: </b><a href="https://cocoxu.github.io/files/20201027_simplification_UPitt.pdf">Download </a></p>
			</div>
			</div>

			<hr>

			<div class="row">
			<div class="col-md-12 text-left">
				<div class="speaker-pic" style="display: inline-block">
					<img src="img/zhouyu.png">
				</div>
				<div>
				<h3> Zhou Yu </h3>
				<p><b> University of California, Davis</b></p>
				<p><b> Time:</b> 11/10/2020, Tuesday from 3 to 4 pm </p>
				<p><b> Place:</b> Zoom</p>
				<p><b> Title:</b> Personalized Persuasive Dialog Systems </p>
				<p><b> Topic:</b> Dialogue Systems </p>
				<p><b>Abstract:</b> Dialog systems such as Alexa and Siri are everywhere in our lives. They can complete tasks such as booking flights, making restaurant reservations and training people for interviews. These systems are passively follow-along human needs.  What if the dialog systems have a different goal than users. We introduce dialog systems that can persuade users to donate to charities. We further improve the dialog model's coherence by tracking both semantic actions and conversational strategies from dialog history using finite-state transducers. Finally, we analyze some ethical concerns and human factors in deploying personalized persuasive dialog systems. </p>
				<p><b>Bio:</b> Zhou Yu is an Assistant Professor at the UC Davis Computer Science Department. Zhou will join the CS department at Columbia University in Jan 2021 as an Assistant Professor. She obtained her Ph.D. from Carnegie Mellon University in 2017.  Zhou has built various dialog systems that have a real impact, such as a job interview training system, a depression screening system, and a second language learning system. Her research interest includes dialog systems, language understanding and generation, vision and language, human-computer interaction, and social robots. Zhou received an ACL 2019 best paper nomination, featured in Forbes 2018 30 under 30 in Science, and won the 2018 Amazon Alexa Prize. </p>
				</div>
			</div>
			</div>

			<hr>

			<div class="row">
			<div class="col-md-12 text-left">
				<div class="speaker-pic" style="display: inline-block">
					<img src="https://jmhessel.com/main_photo.jpg">
				</div>
				<div>
				<h3> Jack Hessel </h3>
				<p><b> AI2 </b></p>
				<p><b> Time:</b> 11/24/2020</p>
				<p><b> Place:</b> Zoom</p>
				<p><b> Title:</b> (at least) Two Conceptions of Visual-Textual Grounding</p>
				<p><b> Topic:</b> Multimodal Communication </p>
				<p><b>Abstract:</b> Algorithms that learn connections between visual and textual content underlie many important applications of AI, e.g., image captioning, robot navigation, and web video parsing. But what does it really mean for images and text to be "connected"? I'll discuss (at least) two orthogonal conceptions of visual-textual grounding. The first is operational grounding: if an algorithm can learn a consistent relationship between two data modalities based on co-occurrence data, then such a pattern can be called "grounded;" I'll discuss our work that applies this notion to both static images and to web videos. The second, more general view describes visual-textual grounding as a subset of "interesting" logical functions that take as input visual and textual variables. Under this paradigm, we design a diagnostic that can tell you if your multimodal model is doing cross-modal reasoning, or (as we find is the common case) exploiting single modal biases.</p>
				<p><b>Bio:</b> Jack is a postdoc at AI2, and earned a PhD in Computer Science at Cornell University. His work focuses on analyzing user-generated web content, and has been published at EMNLP, NAACL, WWW, etc. Previously, he's worked at Google, Facebook, and Twitter, and held an invited visiting faculty position in Computer Science at Carleton College.</p>
				</div>
			</div>
			</div>

			<div class="row">
			<div class="col-md-12 text-left">
				<div class="speaker-pic" style="display: inline-block">
					<img src="img/nasrin-mostafazadeh.jpeg">
				</div>
				<div>
				<h3> Nasrin Mostafazadeh </h3>
				<p><b> Verneek </b></p>
				<p><b> Time:</b> 2/9/2021</p>
				<p><b> Place:</b> Zoom</p>
				<p><b> Title:</b> How far have we come in giving our NLU systems common sense? </p>
				<p><b> Topic:</b> Common Sense Reasoning </p>
				<p><b>Abstract:</b>  Commonsense reasoning has been a long-established area in AI for more than three decades. Despite the lack of much ongoing effort in this area after the 80s, recently there has been a renewed interest in the AI community for giving machines
					common sense, acknowledging it as the holy grail of AI. With the tremendous recent progress in natural language understanding (NLU), the lack of commonsense reasoning capabilities of NLU systems is more evident than ever. In this talk, Iâ€™ll discuss the amazing
					recent progress made in tackling commonsense reasoning benchmarks using the gigantic pre-trained neural models. Iâ€™ll talk about the role of benchmarks in measuring our progress and how we can move the goal post. Constructing coherent mental models of narratives
					that an NLU system reads, through establishing the chain of causality of implicit and explicit events and states, is a promising step forward.</p>
				<p><b>Bio:</b> Nasrin is Co-founder of Verneek, a new AI startup that is striving to enable anyone to make data-informed decisions without needing to have any technical background. Before Verneek, Nasrin held senior research positions at AI startups BenevolentAI
					and Elemental Cognition and earlier at Microsoft Research and Google. She received her PhD at the University of Rochester working at the conversational interaction and dialogue research group under James F. Allen, with her PhD work focused on commonsense reasoning
					through the lens of story understanding. She has started lines of research that push AI toward deeper understanding and having common sense, with applications ranging from storytelling to vision & language. She has been a keynote speaker, chair, organizer,
					and program committee member at different AI venues. Nasrin was named to Forbesâ€™ 30 Under 30 in Science 2019 for her work in AI.

</p>
				</div>
			</div>
			</div>

			<div class="row">
			<div class="col-md-12 text-left">
				<div class="speaker-pic" style="display: inline-block">
					<img src="img/profile.png">
				</div>
				<div>
				<h3> Vered Shwartz</h3>
				<p><b> AI2 </b></p>
				<p><b> Time:</b> 2/23/2021</p>
				<p><b> Place:</b> Zoom</p>
				<p><b> Title:</b> Commonsense Knowledge and Reasoning in Natural Language </p>
				<p><b> Topic:</b> Common Sense Reasoning </p>
				<p><b>Abstract:</b> Natural language understanding models are trained on a sample of the real-world situations they may encounter. Commonsense and world knowledge, language, and reasoning skills can help them address unknown situations sensibly.

In this talk I will present two lines of work addressing commonsense knowledge and reasoning in natural language. I will first present a method for discovering relevant knowledge which is unstated but may be required for solving a particular problem, e.g., to correctly resolve "Children need to eat more vegetables because they [children / vegetables] are healthy" one needs to know that "vegetables are healthy". Such knowledge is discovered through a process of asking information seeking clarification questions (e.g. "what is the purpose of vegetables?") and answering them ("to provide nutrients").

I will then discuss nonmonotonic reasoning in natural language, a core human reasoning ability that has been studied in classical AI but mostly overlooked in modern NLP. I will talk about several recent papers addressing abductive reasoning (reasoning about plausible explanations), counterfactual reasoning (what if?) and defeasible reasoning (updating beliefs given additional information). Finally, I will discuss open problems in language, knowledge, and reasoning.  </p>
				<p><b>Bio:</b> Vered Shwartz is a postdoctoral researcher at the Allen Institute for AI (AI2) and the Paul G. Allen School of Computer Science & Engineering at the University of Washington. Previously, she completed her PhD in Computer Science from Bar-Ilan University, under the supervision of Prof. Ido Dagan. Her research interests include commonsense reasoning, lexical and compositional semantics. </p>
				</div>
			</div>
			</div>

			<div class="row">
			<div class="col-md-12 text-left">
				<div class="speaker-pic" style="display: inline-block">
					<img src="img/jessy-li.png">
				</div>
				<div>
				<h3> Jessy Li </h3>
				<p><b> UT Austin </b></p>
				<p><b> Time:</b> 3/23/2021</p>
				<p><b> Place:</b> Zoom</p>
				<p><b> Title:</b> Help! Need Advice on Discourse Comprehension </p>
				<p><b> Topic:</b> Discourse Comprehension </p>
				<p><b>Abstract:</b> With large-scale pre-trained models, natural language processing as a field has made giant leaps in a wide range of tasks. But how are we doing on those that require a deeper understanding of discourse pragmatics, tasks that we humans use language to accomplish on a daily basis? We discuss a case study of advice giving in online forums, and reveal rich discourse strategies in the language of advice. Understanding advice would equip systems with a better grasp of language pragmatics, yet we show that advice identification is challenging for modern NLP models. So then --- how do people comprehend at the discourse level? We tackle this via a novel question generation paradigm, by capturing questions elicited from readers as they read through a text sentence by sentence. Because these questions are generated while the readers are processing the information, they are naturally inquisitive, with a variety of types such as causal, elaboration, and background. Finally, we briefly showcase a new task that requires high level inferences when the target audience of a document changes: providing elaborations and explanations during text simplification.
				<p><b>Bio:</b> Jessy Li (<a>https://jessyli.com</a>) is an assistant professor in the Department of Linguistics at UT Austin where she works on computational linguistics and natural language processing. Her work focuses on discourse organization, text intelligibility, and language pragmatics in social media. She received her Ph.D. in 2017 from the University of Pennsylvania. She received an ACM SIGSOFT Distinguished Paper Award at FSE 2019, an Area Chair Favorite at COLING 2018, and a Best Paper nomination at SIGDIAL 2016. </p>
				</div>
			</div>
			</div>

			<div class="row">
			<div class="col-md-12 text-left">
				<div class="speaker-pic" style="display: inline-block">
					<img src="https://vivo.brown.edu/profile-images/219/652/49/epavlick_photo_.jpg">
				</div>
				<div>
				<h3> Ellie Pavlick </h3>
				<p><b> Brown University </b></p>
				<p><b> Time:</b> 4/6/2021</p>
				<p><b> Place:</b> Zoom</p>
				<p><b> Title:</b> You can lead a horse to water...: Representing vs. Using Features in Neural NLP
 </p>
				<p><b> Topic:</b> Pretrained Language Models </p>
				<p><b>Abstract:</b> A wave of recent work has sought to understand how pretrained language models work. Such analyses have resulted in two seemingly contradictory sets of results. On one hand, work based on "probing classifiers" generally suggests that SOTA language models contain rich information about linguistic structure (e.g., parts of speech, syntax, semantic roles). On the other hand, work which measures performance on linguistic "challenge sets" shows that models consistently fail to use this information when making predictions. In this talk, I will present a series of results that attempt to bridge this gap. Our recent experiments suggest that the disconnect is not due to catastrophic forgetting nor is it (entirely) explained by insufficient training data. Rather, it is best explained in terms of how "accessible" features are to the model following pretraining, where "accessibility" can be quantified using an information-theoretic interpretation of probing classifiers.
				<p><b>Bio:</b> Ellie Pavlick is an Assistant Professor of Computer Science at Brown University where she leads the Language Understanding and Representation (LUNAR) Lab. She received her PhD from the one-and-only University of Pennsylvania. Her current work focuses on building more cognitively-plausible models of natural language semantics, focusing on grounded language learning and on sample efficiency and generalization of neural language models.
 </p>
				</div>
			</div>
			</div>

			<!---
			<div class="col-md-3 text-center">
				<span class="icons c4">
				<i class="fa fa-globe"></i></span>
				<div class="box-area">
				<h3>User Experiance</h3>
				<p>Nothing</p>
				</div>
			</div>
			-->

		</div></div>
		<!--</div>-->


	</div>
	</section>

<div class="container2">

<form action="mailto:pittnlpseminar@list.pitt.edu" enctype="text/plain" method="POST">

  <div class ="email-box">
		<i class="fa fa-envelope" > </i>
		<input class="tbox" type="email" name="email" value="" placeholder="Enter your e-mail" required>
		<button class="btn" type="submit"  name="button"> Subscribe </button>
  </div>
 </form >
 </div>

	<div id="sub-footer">
		<div class="container">
			<div class="row">
				<div class="col-lg-6">
					<div class="copyright">
						<p>
							<span>&copy; All rights reserved </span>
						</p>
					</div>
				</div>
				<div class="col-lg-6">
					<ul class="social-network">
						<!--
						<li><a href="#" data-placement="top" title="Facebook"><i class="fa fa-facebook"></i></a></li>
						<li><a href="#" data-placement="top" title="Twitter"><i class="fa fa-twitter"></i></a></li>
						<li><a href="#" data-placement="top" title="Linkedin"><i class="fa fa-linkedin"></i></a></li>
						<li><a href="#" data-placement="top" title="Pinterest"><i class="fa fa-pinterest"></i></a></li>
						<li><a href="#" data-placement="top" title="Google plus"><i class="fa fa-google-plus"></i></a></li>
						-->
					</ul>
				</div>
			</div>
		</div>
	</div>
	</footer>
</div>

<!-- Something else -->
<a href="#" class="scrollup"><i class="fa fa-angle-up active"></i></a>
<!-- javascript
    ================================================== -->
<!-- Placed at the end of the document so the pages load faster -->
<script src="js/jquery.js"></script>
<script src="js/jquery.easing.1.3.js"></script>
<script src="js/bootstrap.min.js"></script>
<script src="js/jquery.fancybox.pack.js"></script>
<script src="js/jquery.fancybox-media.js"></script>
<script src="js/portfolio/jquery.quicksand.js"></script>
<script src="js/portfolio/setting.js"></script>
<script src="js/jquery.flexslider.js"></script>
<script src="js/animate.js"></script>
<script src="js/custom.js"></script>
<script src="js/owl-carousel/owl.carousel.js"></script>
</body>
</html>
